{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "- Classification\n",
    "- Regression\n",
    "\n",
    "\n",
    "### Introduction to Linear Regressions\n",
    "\n",
    "**Pros:** fast, no tuning required, highly interpretable, well-understood\n",
    "\n",
    "**Cons:** unlikely to produce the best predictive accuracy (presumes a linear relationship between the features and response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "[Statsmodels](http://statsmodels.sourceforge.net/) introduces nice  characteristics for linear modeling. However, we recommend that you spend most of your energy on [scikit-learn](http://scikit-learn.org/stable/) since it provides significantly more useful functionality for machine learning in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from IPython.display import display\n",
    "\n",
    "# allow plots to appear directly in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Advertising Data\n",
    "\n",
    "Let's take a look at some data, ask some questions about that data, and then use linear regression to answer those questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV file stored in the current directory and save the results\n",
    "data = pd.read_csv('./data/Advertising.csv', skipinitialspace=True, index_col=0)\n",
    "\n",
    "# display the first 5 rows\n",
    "print(data.head())\n",
    "print(data.tail())\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the features?\n",
    "- **TV:** advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "- **Radio:** advertising dollars spent on Radio\n",
    "- **Newspaper:** advertising dollars spent on Newspaper\n",
    "\n",
    "What is the response?\n",
    "- **Sales:** sales of a single product in a given market (in thousands of items)\n",
    "\n",
    "What else do we know?\n",
    "- Because the response variable is continuous, this is a **regression** problem.\n",
    "- There are 200 **observations** (represented by the rows), and each observation is a single market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the DataFrame (rows, columns)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions About the Advertising Data\n",
    "\n",
    "Let's pretend you work for the company that manufactures and markets this widget. The company might ask you the following: On the basis of this data, how should we spend our advertising money in the future?\n",
    "\n",
    "This general question might lead you to more specific questions:\n",
    "1. Is there a relationship between ads and sales?\n",
    "2. How strong is that relationship?\n",
    "3. Which ad types contribute to sales?\n",
    "4. What is the effect of each ad type of sales?\n",
    "5. Given ad spending in a particular market, can sales be predicted?\n",
    "\n",
    "We will explore these questions below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Simple linear regression is an approach for predicting a **quantitative response** using a **single feature** (or \"predictor\" or \"input variable\"). It takes the following form:\n",
    "\n",
    "$y = \\theta_0x_0 + \\theta_1x_1$\n",
    "\n",
    "- $y$ is the response\n",
    "- $x_0 = 1$\n",
    "- $x_1$ is the attribute\n",
    "- $\\theta_0$ is the coeffficient for $x_0$ (intercept)\n",
    "- $\\theta_1$ is the coefficient for $x_1$\n",
    "\n",
    "Together, $\\theta_0$ and $\\theta_1$ are called the **model coefficients**. To create your model, you must \"learn\" the values of these coefficients. And once we've learned these coefficients, we can use the model to predict Sales!\n",
    "\n",
    "\n",
    "### Estimating (\"Learning\") Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):\n",
    "\n",
    "Let's estimate the model coefficients for the advertising data (only using the variables TV and Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Estimating Model using 'TV' and 'Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS (method1) ###\n",
    "\n",
    "# create a fitted model\n",
    "lm1 = smf.ols(formula='Sales ~ TV', data=data).fit()\n",
    "\n",
    "# print the coefficients\n",
    "lm1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCIKIT-LEARN (method2) ###\n",
    "\n",
    "# create X and y\n",
    "feature_cols = ['TV']\n",
    "X = data[feature_cols]\n",
    "y = data['Sales']   \n",
    "\n",
    "# instantiate and fit\n",
    "# Create linear regression object\n",
    "lm2 = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets (this time we use all data)\n",
    "lm2.fit(X, y)\n",
    "\n",
    "# print intercept\n",
    "print ('Intercept: ', lm2.intercept_)\n",
    "          \n",
    "# print the coefficients     \n",
    "print('Coefficients: ', lm2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Model Coefficients\n",
    "\n",
    "How do we interpret the TV coefficient ($\\theta_1$)?\n",
    "- A \"unit\" increase in TV ad spending is **associated with** a 0.047537 \"unit\" increase in Sales.\n",
    "- Or more clearly: An additional $1,000 spent on TV ads is **associated with** an increase in sales of 47.537 widgets.\n",
    "\n",
    "Note that if an increase in TV ad spending was associated with a **decrease** in sales, $\\theta_1$ would be **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model for Prediction\n",
    "\n",
    "Let's say that there was a new market where the TV advertising spend was **$50,000**. What would we predict for the Sales in that market?\n",
    "\n",
    "$$y = \\theta_0 + \\theta_1x$$\n",
    "$$y = 7.032594 + 0.047537 \\times 50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually calculate the prediction\n",
    "7.032594 + 0.047537*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "\n",
    "# you have to create a DataFrame since the Statsmodels formula interface expects it\n",
    "X_new = pd.DataFrame({'TV': [50]})\n",
    "\n",
    "# predict for a new observation\n",
    "lm1.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCIKIT-LEARN ###\n",
    "\n",
    "# predict for a new observation\n",
    "lm2.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data and the regression function\n",
    "\n",
    "Plot scatterplot and the regression function\n",
    "\n",
    "### Estimating Model using 'TV' and 'Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatterplot and the regression function\n",
    "plt.scatter(X, y, color='black', label='observed')\n",
    "\n",
    "plt.xlim([0,300])\n",
    "plt.ylim([0, 30])\n",
    "plt.plot(X, lm2.predict(X), label='fit', color='blue', linewidth=2)\n",
    "\n",
    "plt.xlabel('TV')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Regression')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we would predict Sales of 9,409 widgets in that market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Estimating Model using 'Radio' and 'Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCIKIT-LEARN (method2) ###\n",
    "\n",
    "# create X and y\n",
    "feature_cols = ['Radio']\n",
    "X = data[feature_cols]\n",
    "y = data['Sales']   \n",
    "\n",
    "# instantiate and fit\n",
    "lm2 = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets (this time we use all data)\n",
    "lm2.fit(X, y)\n",
    "\n",
    "# Plot scatterplot and the regression function\n",
    "plt.scatter(X, y, color='black', label='observed')\n",
    "\n",
    "plt.xlim([0, 55])\n",
    "plt.ylim([0, 30])\n",
    "plt.plot(X, lm2.predict(X), label='fit', color='Cyan', linewidth=2)\n",
    "\n",
    "plt.xlabel('Radio')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Regression')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Model using 'Newspaper' and 'Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCIKIT-LEARN (method2) ###\n",
    "\n",
    "# create X and y\n",
    "feature_cols = ['Newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data['Sales']   \n",
    "\n",
    "# instantiate and fit\n",
    "lm2 = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets (this time we use all data)\n",
    "lm2.fit(X, y)\n",
    "\n",
    "# Plot scatterplot and the regression function\n",
    "plt.scatter(X, y, color='black', label='observed')\n",
    "\n",
    "plt.xlim([0,100])\n",
    "plt.ylim([0, 30])\n",
    "plt.plot(X, lm2.predict(X), label='fit', color='Green', linewidth=2)\n",
    "\n",
    "plt.xlabel('Newspaper')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Regression')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Well Does the Model Fit the data?\n",
    "\n",
    "The most common way to evaluate the overall fit of a linear model is by the **R-squared** value. R-squared is the **proportion of variance explained**, meaning the proportion of variance in the observed data that is explained by the model, or the reduction in error over the **null model**. (The null model just predicts the mean of the observed response, and thus it has an intercept and no slope.)\n",
    "\n",
    "R-squared is between 0 and 1, and higher is better because it means that more variance is explained by the model. Here's an example of what R-squared \"looks like\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the R-squared value for our simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "\n",
    "# print the R-squared value for the model\n",
    "lm1.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCIKIT-LEARN ###\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.4f\" % np.mean((lm2.predict(X) - y) ** 2))\n",
    "\n",
    "# print the R-squared value (explained variance score) for the model: 1 is perfect prediction\n",
    "print('Variance score: %.4f' % lm2.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that a \"good\" R-squared value? It's hard to say. The threshold for a good R-squared value depends widely on the domain. Therefore, it's most useful as a tool for **comparing different models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear regression\n",
    "\n",
    "Simple linear regression can easily be extended to include multiple features. This is called **multiple linear regression**:\n",
    "### Form of linear regression\n",
    "\n",
    "$y = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$\n",
    "\n",
    "- $y$ is the response\n",
    "- $x_0 = 1$\n",
    "- $x_1, x_2, ..., x_n$ are attributes\n",
    "- $\\theta_0$ is the coeffficient for $x_0$ (intercept)\n",
    "- $\\theta_1$ is the coefficient for $x_1$ (the first attribute)\n",
    "- $\\theta_n$ is the coefficient for $x_n$ (the nth attribute)\n",
    "\n",
    "The $\\theta$ values are called the **model coefficients**. These values are \"learned\" during the model fitting step using the \"least squares\" criterion. Then, the fitted model can be used to make predictions!\n",
    "\n",
    "In this example, \n",
    "\n",
    "$Sales = \\theta_0x_0 + \\theta_1TV + \\theta_2Radio + \\theta_3Newspaper$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate these coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "\n",
    "# create a fitted model with all three features\n",
    "lm1 = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\n",
    "\n",
    "# print the coefficients\n",
    "lm1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCIKIT-LEARN ###\n",
    "\n",
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data[['Sales']]\n",
    "\n",
    "# instantiate and fit\n",
    "lm2 = linear_model.LinearRegression()\n",
    "lm2.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print ('Intercept: ', lm2.intercept_)\n",
    "print ('Coefficients: ', lm2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "\n",
    "# print a summary of the fitted model\n",
    "lm1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Discussion\n",
    "\n",
    "How do I decide **which features to include** in a linear model? Here's one idea:\n",
    "- Try different models, and only keep features in the model if they have small p-values.\n",
    "- Check whether the R-squared value goes up when you add new features.\n",
    "\n",
    "What are the **drawbacks** to this approach?\n",
    "- Linear models rely upon a lot of **assumptions** (such as the features being independent), and if those assumptions are violated (which they usually are), R-squared and p-values are less reliable.\n",
    "- Using a p-value cutoff of 0.05 means that if you add 100 features to a model that are **pure noise**, 5 of them (on average) will still be counted as significant.\n",
    "- R-squared is susceptible to **overfitting**, and thus there is no guarantee that a model with a high R-squared value will generalize. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "\n",
    "# only include TV and Radio in the model\n",
    "lm1 = smf.ols(formula='Sales ~ TV + Radio', data=data).fit()\n",
    "lm1.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Newspaper to the model (which we believe has no association with Sales)\n",
    "lm1 = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\n",
    "lm1.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R-squared will always increase as you add more features to the model**, even if they are unrelated to the response. Thus, selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "\n",
    "There is alternative to R-squared called **adjusted R-squared** that penalizes model complexity (to control for overfitting), but it generally [under-penalizes complexity](http://scott.fortmann-roe.com/docs/MeasuringError.html).\n",
    "\n",
    "So is there a better approach to feature selection? **Train/test split** or **cross-validation.** They provide a more reliable estimate of out-of-sample error, and thus are better for choosing which of your models will best **generalize** to out-of-sample data. There is extensive functionality for cross-validation in scikit-learn, including automated methods for searching different sets of parameters and different models. Importantly, cross-validation can be applied to **any model**, whereas the methods described above only apply to **linear models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation metrics for regression\n",
    "\n",
    "Evaluation metrics for classification problems, such as **accuracy**, are not useful for regression problems. Instead, we need evaluation metrics designed for comparing continuous values.\n",
    "\n",
    "Let's create some example numeric predictions, and calculate **three common evaluation metrics** for regression problems:\n",
    "\n",
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "\n",
    "Comparing these metrics:\n",
    "\n",
    "- **MAE** is the easiest to understand, because it's the average error.\n",
    "- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors.\n",
    "- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "\n",
    "### The R2 (\"r-squared\") Regression Score\n",
    "\n",
    "- Measures how well a prediction model for regression fits the given data.\n",
    "\n",
    "- The score is between 0 and 1:\n",
    "\n",
    "     - A value of 0 corresponds to a constant model that predicts the mean value of all training target values.\n",
    "\n",
    "     - A value of 1 corresponds to perfect prediction\n",
    "\n",
    "- Also known as \"coefficient of determination\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple examples for evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# define true and predicted response values\n",
    "true = [100, 50, 30, 20]\n",
    "pred = [90, 50, 50, 30]\n",
    "\n",
    "\n",
    "print('calculate MAE by hand')\n",
    "print((10 + 0 + 20 + 10)/4.)\n",
    "\n",
    "print('calculate MAE using scikit-learn')\n",
    "print(metrics.mean_absolute_error(true, pred))\n",
    "\n",
    "\n",
    "print('calculate MSE by hand')\n",
    "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\n",
    "\n",
    "print('calculate MSE using scikit-learn')\n",
    "print(metrics.mean_squared_error(true, pred))\n",
    "\n",
    "print('calculate RMSE by hand')\n",
    "import numpy as np\n",
    "print(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\n",
    "\n",
    "print('calculate RMSE using scikit-learn')\n",
    "print(metrics.mean_squared_error(true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our example of the regression for 'TV' and 'Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the testing set\n",
    "y_pred = lm2.predict(X)\n",
    "\n",
    "# compute the RMSE of our predictions\n",
    "print(np.sqrt(metrics.mean_squared_error(y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression Analysis on Advertising Data\n",
    "\n",
    "### 1. Preparing X and Y using pandas\n",
    "\n",
    "- scikit-learn expects X (feature matrix) and Y (response vector) to be NumPy arrays.\n",
    "- However, pandas is built on top of NumPy.\n",
    "- Thus, X can be a pandas DataFrame and y can be a pandas Series!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python list of feature names\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "\n",
    "# use the list to select a subset of the original DataFrame\n",
    "X = data[feature_cols]\n",
    "\n",
    "# equivalent command to do the above in one line\n",
    "X = data[['TV', 'Radio', 'Newspaper']]\n",
    "\n",
    "# print the first 5 rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type and shape of X\n",
    "print(type(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a Series from the DataFrame\n",
    "y = data['Sales']\n",
    "\n",
    "# print the first 5 values\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type and shape of y\n",
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting X and y into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default split is 75% for training and 25% for testing\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear regression in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "linreg = linear_model.LinearRegression()\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the intercept and coefficients\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the feature names with the coefficients\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = 2.88 + 0.0466 \\times TV + 0.179 \\times Radio + 0.00345 \\times Newspaper$$\n",
    "\n",
    "How do we interpret the **TV coefficient** (0.0466)?\n",
    "\n",
    "- For a given amount of Radio and Newspaper ad spending, **a \"unit\" increase in TV ad spending** is associated with a **0.0466 \"unit\" increase in Sales**.\n",
    "- Or more clearly: For a given amount of Radio and Newspaper ad spending, **an additional $1,000 spent on TV ads** is associated with an **increase in sales of 46.6 items**.\n",
    "\n",
    "Important notes:\n",
    "\n",
    "- This is a statement of **association**, not **causation**.\n",
    "- If an increase in TV ad spending was associated with a **decrease** in sales, $\\beta_1$ would be **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an **evaluation metric** in order to compare our predictions with the actual values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing the RMSE for our Sales predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is lower than the RMSE value for the regression where we use only TV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature selection consideration\n",
    "\n",
    "Does **Newspaper** \"belong\" in our model? In other words, does it improve the quality of our predictions?\n",
    "\n",
    "Let's **remove it** from the model and check the RMSE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python list of feature names\n",
    "feature_cols = ['TV', 'Radio']\n",
    "\n",
    "# use the list to select a subset of the original DataFrame\n",
    "X = data[feature_cols]\n",
    "\n",
    "# select a Series from the DataFrame\n",
    "y = data.Sales\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "# compute the RMSE of our predictions\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE **decreased** when we removed Newspaper from the model. (Error is something we want to minimize, so **a lower number for RMSE is better**.) Thus, it is unlikely that this feature is useful for predicting Sales, and should be removed from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(linreg, X, y, cv=3,\n",
    "                         scoring=('r2', 'neg_mean_squared_error'),\n",
    "                         return_train_score=True)\n",
    "\n",
    "print(scores['test_neg_mean_squared_error'])\n",
    "\n",
    "print(scores['train_r2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Linear regression:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Cross validation:\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "Performance metrics:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
